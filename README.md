# Summarization of Web pages
Follow this link to [view the code](https://colab.research.google.com/drive/1sWqECd9cK7-lw9Zk_i3AXK2CzXOocELM?usp=sharing)

## Background
Whether using Google or another search engine, we frequently discover that the results are dispersed over numerous webpages. Alternatively, we can come upon a scenario where, after looking through a whole webpage, we discover that it lacks the information we were seeking. Therefore, a system that gathers and summarizes many webpages automatically would save time and effort while removing advertisements, filler text, and other unimportant elements from the actual content.
## Workflow
Python documents were loaded and the top 'n' webpages for a given query were called automatically using Selenium. After that, the HTML is cleaned, parsed, and combined into a single document. Before determining the best fit, we experimented with a variety of summarization strategies and models, including Deep Learning methods employing Transformers and algorithmic approaches like SVD. While the procedure for gathering text data from a user query is the same for all approaches, each method has different pre-processing processes and output. BART is used for summarizing in the URL provided at the top. To view the outcome of an extractive summarization using Singular Value Decomposition and learn how an extractive model functions, click [this link](https://colab.research.google.com/drive/1GhtG3qJLQ2IxeMhDrKc8oWLR4g2iPBf0?usp=sharing). The code for each of the aforementioned stepsâ€”from information extraction to summarization can be found at the provided links. It is possible to manually compare the results produced by the Extractive and Abstractive techniques.
## Model Training
To train and optimize the BERT model for summarization, we used [this repository](https://github.com/joshcoward63/TextAbstractiveSummarization). To view the model training program, click [this link](https://colab.research.google.com/drive/1iz2YMzVzSt8_6GtL7l9m-TyzyCI2Waes?usp=sharing). Since the aforementioned repository is outdated and contains many obsolete libraries, we closely followed it as a guide for our work and made various changes to the application. In order to train the model more quickly, the parameters were also altered, which would lower the accuracy.
